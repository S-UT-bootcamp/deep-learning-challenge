{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd \n",
    "application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "\n",
    "application_df.drop(['EIN', 'NAME'], axis=1, inplace=True)\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "unique_value_counts = application_df.nunique()\n",
    "print(unique_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning\n",
    "application_type_counts = application_df['APPLICATION_TYPE'].value_counts()\n",
    "application_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cutoff value\n",
    "cutoff_value = 500  # You can adjust this cutoff value as needed\n",
    "\n",
    "# Get the value counts of application types\n",
    "application_type_counts = application_df['APPLICATION_TYPE'].value_counts()\n",
    "\n",
    "# Identify application types to be replaced with \"Other\"\n",
    "application_types_to_replace = application_type_counts[application_type_counts < cutoff_value].index\n",
    "\n",
    "# Replace in the DataFrame\n",
    "application_df['APPLICATION_TYPE'].replace(application_types_to_replace, \"Other\", inplace=True)\n",
    "\n",
    "# Check to make sure binning was successful\n",
    "print(application_df['APPLICATION_TYPE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at CLASSIFICATION value counts for binning\n",
    "application_df['CLASSIFICATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may find it helpful to look at CLASSIFICATION value counts >1\n",
    "\n",
    "classification_counts=application_df['CLASSIFICATION'].value_counts().loc[lambda x : x >1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cutoff value (e.g., 1000) for classification counts\n",
    "cutoff_value = 1000\n",
    "\n",
    "# Get the counts of each classification\n",
    "classification_counts = application_df['CLASSIFICATION'].value_counts()\n",
    "\n",
    "# Create a list of classifications to be replaced based on the cutoff value\n",
    "classifications_to_replace = classification_counts[classification_counts < cutoff_value].index.tolist()\n",
    "\n",
    "\n",
    "# Replace in dataframe\n",
    "for cls in classifications_to_replace:\n",
    "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "application_df['CLASSIFICATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "\n",
    "df = pd.DataFrame(application_df)\n",
    "\n",
    "# Convert 'Category' column to dummy variables\n",
    "df = pd.get_dummies(df).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "# Assuming you have a DataFrame 'application_df' with preprocessed data\n",
    "\n",
    "# Define your features (X) and target (y)\n",
    "X = df.drop(columns=['IS_SUCCESSFUL'],axis=1)\n",
    "y = df['IS_SUCCESSFUL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "804/804 [==============================] - 1s 663us/step - loss: 0.5925 - accuracy: 0.7076\n",
      "Epoch 2/10\n",
      "804/804 [==============================] - 1s 655us/step - loss: 0.5743 - accuracy: 0.7215\n",
      "Epoch 3/10\n",
      "804/804 [==============================] - 1s 661us/step - loss: 0.5661 - accuracy: 0.7250\n",
      "Epoch 4/10\n",
      "804/804 [==============================] - 1s 637us/step - loss: 0.5615 - accuracy: 0.7277\n",
      "Epoch 5/10\n",
      "804/804 [==============================] - 1s 642us/step - loss: 0.5591 - accuracy: 0.7298\n",
      "Epoch 6/10\n",
      "804/804 [==============================] - 1s 645us/step - loss: 0.5576 - accuracy: 0.7300\n",
      "Epoch 7/10\n",
      "804/804 [==============================] - 1s 659us/step - loss: 0.5566 - accuracy: 0.7305\n",
      "Epoch 8/10\n",
      "804/804 [==============================] - 1s 680us/step - loss: 0.5553 - accuracy: 0.7304\n",
      "Epoch 9/10\n",
      "804/804 [==============================] - 1s 668us/step - loss: 0.5547 - accuracy: 0.7307\n",
      "Epoch 10/10\n",
      "804/804 [==============================] - 1s 655us/step - loss: 0.5544 - accuracy: 0.7311\n",
      "268/268 - 0s - loss: 0.5571 - accuracy: 0.7262 - 186ms/epoch - 693us/step\n",
      "Optimizer: RMSprop\n",
      "Loss: 0.5571078658103943, Accuracy: 0.7261807322502136\n",
      "Epoch 1/10\n",
      "804/804 [==============================] - 1s 664us/step - loss: 0.5512 - accuracy: 0.7322\n",
      "Epoch 2/10\n",
      "804/804 [==============================] - 1s 643us/step - loss: 0.5511 - accuracy: 0.7333\n",
      "Epoch 3/10\n",
      "804/804 [==============================] - 1s 667us/step - loss: 0.5510 - accuracy: 0.7327\n",
      "Epoch 4/10\n",
      "804/804 [==============================] - 1s 671us/step - loss: 0.5509 - accuracy: 0.7325\n",
      "Epoch 5/10\n",
      "804/804 [==============================] - 1s 622us/step - loss: 0.5509 - accuracy: 0.7324\n",
      "Epoch 6/10\n",
      "804/804 [==============================] - 1s 634us/step - loss: 0.5508 - accuracy: 0.7322\n",
      "Epoch 7/10\n",
      "804/804 [==============================] - 1s 638us/step - loss: 0.5508 - accuracy: 0.7329\n",
      "Epoch 8/10\n",
      "804/804 [==============================] - 1s 661us/step - loss: 0.5507 - accuracy: 0.7330\n",
      "Epoch 9/10\n",
      "804/804 [==============================] - 1s 639us/step - loss: 0.5507 - accuracy: 0.7329\n",
      "Epoch 10/10\n",
      "804/804 [==============================] - 1s 646us/step - loss: 0.5508 - accuracy: 0.7325\n",
      "268/268 - 0s - loss: 0.5566 - accuracy: 0.7257 - 179ms/epoch - 669us/step\n",
      "Optimizer: sgd\n",
      "Loss: 0.5565654635429382, Accuracy: 0.7257142663002014\n",
      "Epoch 1/10\n",
      "804/804 [==============================] - 1s 701us/step - loss: 0.5535 - accuracy: 0.7307\n",
      "Epoch 2/10\n",
      "804/804 [==============================] - 1s 706us/step - loss: 0.5524 - accuracy: 0.7318\n",
      "Epoch 3/10\n",
      "804/804 [==============================] - 1s 667us/step - loss: 0.5512 - accuracy: 0.7324\n",
      "Epoch 4/10\n",
      "804/804 [==============================] - 1s 664us/step - loss: 0.5506 - accuracy: 0.7315\n",
      "Epoch 5/10\n",
      "804/804 [==============================] - 1s 660us/step - loss: 0.5502 - accuracy: 0.7329\n",
      "Epoch 6/10\n",
      "804/804 [==============================] - 1s 668us/step - loss: 0.5498 - accuracy: 0.7336\n",
      "Epoch 7/10\n",
      "804/804 [==============================] - 1s 715us/step - loss: 0.5491 - accuracy: 0.7335\n",
      "Epoch 8/10\n",
      "804/804 [==============================] - 1s 660us/step - loss: 0.5487 - accuracy: 0.7331\n",
      "Epoch 9/10\n",
      "804/804 [==============================] - 1s 656us/step - loss: 0.5480 - accuracy: 0.7338\n",
      "Epoch 10/10\n",
      "804/804 [==============================] - 1s 654us/step - loss: 0.5481 - accuracy: 0.7336\n",
      "268/268 - 0s - loss: 0.5559 - accuracy: 0.7272 - 175ms/epoch - 652us/step\n",
      "Optimizer: Adam\n",
      "Loss: 0.555928647518158, Accuracy: 0.7272303104400635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load your dataset and preprocess it as needed\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test = ...\n",
    "# nn_model = tf.keras.models.Sequential()\n",
    "# Define the neural network model\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Hidden layers\n",
    "nn_model.add(Dense(units=80, activation='sigmoid', input_dim=43))\n",
    "nn_model.add(Dense(units=30, activation='sigmoid')) # Fully connected layer with ReLU activation\n",
    "nn_model.add(Dense(units=1, activation='sigmoid'))  # Fully connected layer with ReLU activation\n",
    "# Output layer\n",
    "# nn_model.add(10, activation='softmax')  # Output layer with softmax activation (e.g., for classification)\n",
    "\n",
    "\n",
    "# Compile the model with different optimization methods\n",
    "# optimizers = {\n",
    "# 'sgd': tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "# 'Adam': tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "# 'RMSprop': tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "# }\n",
    "optimizers = {\n",
    "'sgd',\n",
    "'Adam',\n",
    "'RMSprop'\n",
    "}\n",
    "# nn_model.summary()\n",
    "# Iterate through different optimizers and train the model\n",
    "for optimizer_name in optimizers:\n",
    "    nn_model.compile(optimizer=optimizer_name, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # Train the model\n",
    "    # nn_model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "    fit_model = nn_model.fit(X_train_scaled, y_train, epochs=10)\n",
    "    # Evaluate the model on the test data\n",
    "    model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "\n",
    "    print(f\"Optimizer: {optimizer_name}\")\n",
    "    print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ModelCheckpoint callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='model_weights_epoch.h5',\n",
    "    save_weights_only=True,\n",
    "    period=5  # Save weights every 5 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export our model to HDF5 file\n",
    "# nn_model.save(\"AlphabetSoupCharity.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
